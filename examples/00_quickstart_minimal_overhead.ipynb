{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f85b1e5",
   "metadata": {},
   "source": [
    "# SpaX Quickstart: Minimal Overhead Migration\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook demonstrates how to use SpaX for search space exploration with minimal code changes from standard Pydantic configurations.\n",
    "\n",
    "**What you'll learn:**\n",
    "- Migrating from Pydantic `BaseModel` to SpaX `Config` (one line change)\n",
    "- Automatic space inference from type hints and Field constraints\n",
    "- Random sampling and parameter inspection\n",
    "- Explicit space control when needed\n",
    "- Basic search space statistics\n",
    "\n",
    "**Prerequisites:**\n",
    "- Basic Python knowledge\n",
    "- Familiarity with type hints (helpful but not required)\n",
    "- No prior Pydantic or SpaX knowledge needed\n",
    "\n",
    "Let's start by defining a simple machine learning configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa0f8713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden_dim=256 learning_rate=0.001 activation='relu' optimizer='adam' use_batch_norm=True dropout=0.1\n"
     ]
    }
   ],
   "source": [
    "# First, let's see a standard Pydantic configuration\n",
    "from typing import Literal\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class MLConfigPydantic(BaseModel):\n",
    "    \"\"\"A typical Pydantic configuration for ML experiments.\"\"\"\n",
    "\n",
    "    # Numeric parameters with constraints\n",
    "    hidden_dim: int = Field(gt=16, lt=4096)\n",
    "    learning_rate: float = Field(ge=1e-5, le=1e-1)\n",
    "\n",
    "    # Categorical choices\n",
    "    activation: Literal[\"relu\", \"gelu\", \"silu\"]\n",
    "    optimizer: Literal[\"adam\", \"sgd\", \"rmsprop\"]\n",
    "\n",
    "    # Boolean flag\n",
    "    use_batch_norm: bool\n",
    "\n",
    "    # Parameter with default\n",
    "    dropout: float = Field(ge=0.0, le=0.5, default=0.1)\n",
    "\n",
    "\n",
    "# Create an instance manually\n",
    "config = MLConfigPydantic(\n",
    "    hidden_dim=256,\n",
    "    learning_rate=0.001,\n",
    "    activation=\"relu\",\n",
    "    optimizer=\"adam\",\n",
    "    use_batch_norm=True,\n",
    ")\n",
    "\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0ee1d2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLConfigSpaX(hidden_dim=256, learning_rate=0.001, activation='relu', optimizer='adam', use_batch_norm=True, dropout=0.1)\n"
     ]
    }
   ],
   "source": [
    "# Now, let's migrate to SpaX - just change BaseModel to Config!\n",
    "import spax as sp\n",
    "\n",
    "\n",
    "class MLConfigSpaX(sp.Config):\n",
    "    \"\"\"Same configuration, now with search space superpowers.\"\"\"\n",
    "\n",
    "    # Exact same field definitions - no changes needed!\n",
    "    hidden_dim: int = Field(gt=16, lt=4096)\n",
    "    learning_rate: float = Field(ge=1e-5, le=1e-1)\n",
    "    activation: Literal[\"relu\", \"gelu\", \"silu\"]\n",
    "    optimizer: Literal[\"adam\", \"sgd\", \"rmsprop\"]\n",
    "    use_batch_norm: bool\n",
    "    dropout: float = Field(ge=0.0, le=0.5, default=0.1)\n",
    "\n",
    "\n",
    "# Still works exactly like Pydantic\n",
    "config = MLConfigSpaX(\n",
    "    hidden_dim=256,\n",
    "    learning_rate=0.001,\n",
    "    activation=\"relu\",\n",
    "    optimizer=\"adam\",\n",
    "    use_batch_norm=True,\n",
    ")\n",
    "\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d558102c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üé≤ Randomly sampled configuration:\n",
      "MLConfigSpaX(hidden_dim=2636, learning_rate=0.011141993505886382, activation='silu', optimizer='adam', use_batch_norm=True, dropout=0.05124758808575375)\n",
      "\n",
      "üìã Searchable parameters:\n",
      "  ‚Ä¢ MLConfigSpaX.hidden_dim\n",
      "  ‚Ä¢ MLConfigSpaX.learning_rate\n",
      "  ‚Ä¢ MLConfigSpaX.activation\n",
      "  ‚Ä¢ MLConfigSpaX.optimizer\n",
      "  ‚Ä¢ MLConfigSpaX.use_batch_norm\n",
      "  ‚Ä¢ MLConfigSpaX.dropout\n",
      "\n",
      "üîç Search space structure:\n",
      "  hidden_dim: {'gt': 16, 'lt': 4096}\n",
      "  learning_rate: {'ge': 1e-05, 'le': 0.1}\n",
      "  activation: ['relu', 'gelu', 'silu']\n",
      "  optimizer: ['adam', 'sgd', 'rmsprop']\n",
      "  use_batch_norm: ['True', 'False']\n",
      "  dropout: {'ge': 0.0, 'le': 0.5}\n"
     ]
    }
   ],
   "source": [
    "# But now you have NEW capabilities with zero extra code!\n",
    "\n",
    "# 1. Random sampling for testing and exploration\n",
    "random_config = MLConfigSpaX.random(seed=42)\n",
    "print(\"üé≤ Randomly sampled configuration:\")\n",
    "print(random_config)\n",
    "print()\n",
    "\n",
    "# 2. Inspect all searchable parameters\n",
    "print(\"üìã Searchable parameters:\")\n",
    "params = MLConfigSpaX.get_parameter_names()\n",
    "for param in params:\n",
    "    print(f\"  ‚Ä¢ {param}\")\n",
    "print()\n",
    "\n",
    "# 3. Get the override template to see the search space structure\n",
    "print(\"üîç Search space structure:\")\n",
    "template = MLConfigSpaX.get_override_template()\n",
    "for key, value in template.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0009d4",
   "metadata": {},
   "source": [
    "## What Just Happened? Automatic Space Inference\n",
    "\n",
    "SpaX automatically converted your type hints and Pydantic constraints into search spaces:\n",
    "\n",
    "| Type Annotation | Inferred Space |\n",
    "|----------------|----------------|\n",
    "| `Literal[\"relu\", \"gelu\", \"silu\"]` | `CategoricalSpace` with 3 choices |\n",
    "| `bool` | `CategoricalSpace` with `[True, False]` |\n",
    "| `Field(gt=16, lt=4096)` | `IntSpace` with bounds (16, 4096) - exclusive |\n",
    "| `Field(ge=1e-5, le=1e-1)` | `FloatSpace` with bounds [1e-5, 1e-1] - inclusive |\n",
    "\n",
    "**Automatic inference works for:**\n",
    "- ‚úÖ `Literal` types ‚Üí Categorical choices\n",
    "- ‚úÖ `bool` ‚Üí Categorical `[True, False]`\n",
    "- ‚úÖ `Union` / `|` types ‚Üí Categorical choices (only if all args are: `Literal`, `bool`, `None`, or `Config` types)\n",
    "- ‚úÖ Numeric `Field()` with **both** lower and upper bounds ‚Üí Numeric spaces\n",
    "\n",
    "**Limitations:**\n",
    "- ‚ùå Numeric fields without bounds (e.g., `int` alone) - cannot infer range\n",
    "- ‚ùå `Union` with complex types (e.g., `int | str`) - cannot infer\n",
    "- ‚ùå Fields without defaults and not searchable - must provide explicitly\n",
    "\n",
    "**Important:** If you provide a default value for a field without a searchable space definition, it becomes a **fixed value** (not part of the search space).\n",
    "\n",
    "**Solution:** Use explicit SpaX spaces when automatic inference isn't sufficient or for better control.\n",
    "\n",
    "Let's explore what we can do with random sampling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9aff3e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üé≤ Sampling 5 different configurations:\n",
      "\n",
      "Sample 1:\n",
      "  hidden_dim=2636, lr=0.011142, activation=silu, optimizer=adam, batch_norm=True\n",
      "\n",
      "Sample 2:\n",
      "  hidden_dim=174, lr=0.028616, activation=silu, optimizer=rmsprop, batch_norm=True\n",
      "\n",
      "Sample 3:\n",
      "  hidden_dim=1690, lr=0.052007, activation=silu, optimizer=adam, batch_norm=True\n",
      "\n",
      "Sample 4:\n",
      "  hidden_dim=1130, lr=0.041775, activation=relu, optimizer=adam, batch_norm=True\n",
      "\n",
      "Sample 5:\n",
      "  hidden_dim=3655, lr=0.007648, activation=relu, optimizer=sgd, batch_norm=False\n",
      "\n",
      "üîÑ Reproducibility - same seed gives same config:\n",
      "Config 1: hidden_dim=3217\n",
      "Config 2: hidden_dim=3217\n",
      "Equal? True\n"
     ]
    }
   ],
   "source": [
    "# Let's generate multiple random configurations to see the variety\n",
    "print(\"üé≤ Sampling 5 different configurations:\\n\")\n",
    "\n",
    "for i in range(5):\n",
    "    config = MLConfigSpaX.random(seed=42 + i)\n",
    "    print(f\"Sample {i+1}:\")\n",
    "    print(f\"  hidden_dim={config.hidden_dim}, lr={config.learning_rate:.6f}, \"\n",
    "          f\"activation={config.activation}, optimizer={config.optimizer}, \"\n",
    "          f\"batch_norm={config.use_batch_norm}\")\n",
    "    print()\n",
    "\n",
    "# Sampling is reproducible with the same seed\n",
    "print(\"üîÑ Reproducibility - same seed gives same config:\")\n",
    "config1 = MLConfigSpaX.random(seed=999)\n",
    "config2 = MLConfigSpaX.random(seed=999)\n",
    "print(f\"Config 1: hidden_dim={config1.hidden_dim}\")\n",
    "print(f\"Config 2: hidden_dim={config2.hidden_dim}\")\n",
    "print(f\"Equal? {config1.hidden_dim == config2.hidden_dim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c69f13",
   "metadata": {},
   "source": [
    "## Taking Explicit Control\n",
    "\n",
    "While automatic inference is convenient, sometimes you need more control:\n",
    "\n",
    "**When to use explicit SpaX spaces:**\n",
    "- **Distribution control**: Use log distribution for learning rates\n",
    "- **Weighted choices**: Make some categorical options more likely\n",
    "- **Complex conditions**: Parameters that depend on other parameters (covered later)\n",
    "- **Clarity**: When you want the search space to be immediately obvious\n",
    "\n",
    "Let's see explicit spaces in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7a5c784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled configuration:\n",
      "MLConfigExplicit(learning_rate=0.003611662782147249, hidden_dim=119, optimizer='sgd', activation='relu', use_batch_norm=True, model_name='explicit_mlp')\n",
      "\n",
      "Learning rate: 3.61e-03 (log scale)\n",
      "Model name: explicit_mlp (fixed)\n"
     ]
    }
   ],
   "source": [
    "# Example with explicit SpaX spaces for more control\n",
    "\n",
    "\n",
    "class MLConfigExplicit(sp.Config):\n",
    "    \"\"\"Configuration with explicit SpaX spaces.\"\"\"\n",
    "\n",
    "    # Log distribution for learning rate (better for HPO)\n",
    "    learning_rate: float = sp.Float(ge=1e-5, le=1e-1, distribution=\"log\")\n",
    "\n",
    "    # Explicit integer space\n",
    "    hidden_dim: int = sp.Int(gt=16, lt=4096, distribution=\"uniform\")\n",
    "\n",
    "    # Weighted categorical - make \"adam\" more likely\n",
    "    optimizer: str = sp.Categorical(\n",
    "        [\n",
    "            sp.Choice(\"adam\", weight=3.0),  # 3x more likely\n",
    "            sp.Choice(\"sgd\", weight=1.0),\n",
    "            sp.Choice(\"rmsprop\", weight=1.0),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Simple categorical\n",
    "    activation: str = sp.Categorical([\"relu\", \"gelu\", \"silu\"])\n",
    "\n",
    "    # Boolean (could use bool type, but explicit is clearer)\n",
    "    use_batch_norm: bool = sp.Categorical([True, False])\n",
    "\n",
    "    # Fixed value (not part of search space)\n",
    "    model_name: str = \"explicit_mlp\"\n",
    "\n",
    "\n",
    "# Sample and inspect\n",
    "config = MLConfigExplicit.random(seed=42)\n",
    "print(\"Sampled configuration:\")\n",
    "print(config)\n",
    "print(f\"\\nLearning rate: {config.learning_rate:.2e} (log scale)\")\n",
    "print(f\"Model name: {config.model_name} (fixed)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22bab87",
   "metadata": {},
   "source": [
    "## üéØ The Override System: Iterative Space Refinement\n",
    "\n",
    "One of SpaX's most powerful features is the **override system** - it allows you to progressively narrow your search space based on experimental results.\n",
    "\n",
    "**Common workflow:**\n",
    "1. Start with a broad search space\n",
    "2. Run initial experiments\n",
    "3. Identify promising regions\n",
    "4. Create an override to focus on those regions\n",
    "5. Repeat until satisfied\n",
    "\n",
    "**What you can override:**\n",
    "- Numeric bounds: Narrow the range\n",
    "- Categorical choices: Remove unpromising options\n",
    "- Fix parameters: Lock to a specific value\n",
    "\n",
    "Let's see it in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "125b06f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Override template for MLConfigExplicit:\n",
      "{\n",
      "  \"learning_rate\": {\n",
      "    \"ge\": 1e-05,\n",
      "    \"le\": 0.1\n",
      "  },\n",
      "  \"hidden_dim\": {\n",
      "    \"gt\": 16,\n",
      "    \"lt\": 4096\n",
      "  },\n",
      "  \"optimizer\": [\n",
      "    \"adam\",\n",
      "    \"sgd\",\n",
      "    \"rmsprop\"\n",
      "  ],\n",
      "  \"activation\": [\n",
      "    \"relu\",\n",
      "    \"gelu\",\n",
      "    \"silu\"\n",
      "  ],\n",
      "  \"use_batch_norm\": [\n",
      "    \"True\",\n",
      "    \"False\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Get the override template to see the structure\n",
    "import json\n",
    "\n",
    "print(\"üìã Override template for MLConfigExplicit:\")\n",
    "template = MLConfigExplicit.get_override_template()\n",
    "print(json.dumps(template, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b978c640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1: Narrowing numeric ranges\n",
      "--------------------------------------------------\n",
      "Learning rate: 1.90e-03 (now in [1e-4, 1e-2])\n",
      "Hidden dim: 140 (now in [128, 512])\n",
      "\n",
      "Example 2: Removing categorical choices\n",
      "--------------------------------------------------\n",
      "Optimizer: adam (always 'adam' now)\n",
      "Activation: gelu (only 'relu' or 'gelu')\n",
      "\n",
      "Example 3: Mix of fixed and narrowed parameters\n",
      "--------------------------------------------------\n",
      "Learning rate: 0.001 (fixed)\n",
      "Optimizer: adam (fixed)\n",
      "Hidden dim: 313 (still exploring [256, 512])\n"
     ]
    }
   ],
   "source": [
    "# Example 1: Narrow numeric ranges based on experiments\n",
    "override_narrow = {\n",
    "    \"learning_rate\": {\"ge\": 1e-4, \"le\": 1e-2},  # Focus on promising range\n",
    "    \"hidden_dim\": {\"ge\": 128, \"le\": 512},        # Narrow from (16, 4096)\n",
    "}\n",
    "\n",
    "print(\"Example 1: Narrowing numeric ranges\")\n",
    "print(\"-\" * 50)\n",
    "config1 = MLConfigExplicit.random(seed=42, override=override_narrow)\n",
    "print(f\"Learning rate: {config1.learning_rate:.2e} (now in [1e-4, 1e-2])\")\n",
    "print(f\"Hidden dim: {config1.hidden_dim} (now in [128, 512])\")\n",
    "print()\n",
    "\n",
    "# Example 2: Remove unpromising categorical choices\n",
    "override_categorical = {\n",
    "    \"optimizer\": [\"adam\"],           # Only use adam (best performer)\n",
    "    \"activation\": [\"relu\", \"gelu\"],  # Remove silu (worst performer)\n",
    "}\n",
    "\n",
    "print(\"Example 2: Removing categorical choices\")\n",
    "print(\"-\" * 50)\n",
    "config2 = MLConfigExplicit.random(seed=42, override=override_categorical)\n",
    "print(f\"Optimizer: {config2.optimizer} (always 'adam' now)\")\n",
    "print(f\"Activation: {config2.activation} (only 'relu' or 'gelu')\")\n",
    "print()\n",
    "\n",
    "# Example 3: Fix some parameters completely\n",
    "override_fix = {\n",
    "    \"learning_rate\": 0.001,           # Fix to best value found\n",
    "    \"optimizer\": [\"adam\"],             # Fix optimizer\n",
    "    \"hidden_dim\": {\"ge\": 256, \"le\": 512},  # Still explore this\n",
    "}\n",
    "\n",
    "print(\"Example 3: Mix of fixed and narrowed parameters\")\n",
    "print(\"-\" * 50)\n",
    "config3 = MLConfigExplicit.random(seed=42, override=override_fix)\n",
    "print(f\"Learning rate: {config3.learning_rate} (fixed)\")\n",
    "print(f\"Optimizer: {config3.optimizer} (fixed)\")\n",
    "print(f\"Hidden dim: {config3.hidden_dim} (still exploring [256, 512])\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1e0ea7",
   "metadata": {},
   "source": [
    "## üöÄ What's Next? More Power Awaits\n",
    "\n",
    "You've learned the basics, but SpaX can do much more:\n",
    "\n",
    "### Conditional Parameters\n",
    "Parameters that depend on other parameters:\n",
    "```python\n",
    "class ConfigWithConditions(sp.Config):\n",
    "    use_dropout: bool\n",
    "    dropout_rate: float = sp.Conditional(\n",
    "        sp.FieldCondition(\"use_dropout\", sp.EqualsTo(True)),\n",
    "        true=sp.Float(gt=0.0, lt=0.5),\n",
    "        false=0.0  # Fixed when use_dropout=False\n",
    "    )\n",
    "```\n",
    "\n",
    "### Nested Configurations\n",
    "Compose complex configurations from smaller ones:\n",
    "```python\n",
    "class TrainingConfig(sp.Config):\n",
    "    learning_rate: float = sp.Float(ge=1e-5, le=1e-1, distribution=\"log\")\n",
    "    \n",
    "class ModelConfig(sp.Config):\n",
    "    num_layers: int = sp.Int(ge=1, le=10)\n",
    "\n",
    "class ExperimentConfig(sp.Config):\n",
    "    training: TrainingConfig\n",
    "    model: ModelConfig\n",
    "```\n",
    "\n",
    "### HPO Integration (Optuna)\n",
    "```python\n",
    "def objective(trial):\n",
    "    config = MyConfig.from_trial(trial)\n",
    "    return train_and_evaluate(config)\n",
    "\n",
    "study = optuna.create_study()\n",
    "study.optimize(objective, n_trials=100)\n",
    "```\n",
    "\n",
    "### Serialization\n",
    "Save and load configurations in multiple formats:\n",
    "```python\n",
    "# JSON, YAML, TOML support\n",
    "json_str = config.model_dump_json()\n",
    "loaded = MyConfig.model_validate_json(json_str)\n",
    "```\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4904541e",
   "metadata": {},
   "source": [
    "## üìù Summary: What You've Learned\n",
    "\n",
    "In this notebook, you learned how to use SpaX with minimal overhead:\n",
    "\n",
    "### ‚úÖ Core Concepts\n",
    "1. **One-line migration**: `BaseModel` ‚Üí `sp.Config`\n",
    "2. **Automatic inference**: Type hints and `Field()` constraints become search spaces\n",
    "3. **Random sampling**: `Config.random(seed=42)` for testing and exploration\n",
    "4. **Explicit spaces**: Full control with log distributions, weighted choices, and more\n",
    "5. **Override system**: Iteratively narrow search spaces based on results\n",
    "\n",
    "### ‚úÖ What's Possible (Advanced Features)\n",
    "- **Conditional spaces**: Parameters that only exist when conditions are met\n",
    "- **Nested configurations**: Compose complex configs from smaller ones\n",
    "- **HPO integration**: Direct Optuna/trial support\n",
    "- **Multiple serialization formats**: JSON, YAML, TOML\n",
    "\n",
    "### ‚úÖ When to Use What\n",
    "- **Automatic inference**: Quick start, simple constraints, prototyping\n",
    "- **Explicit spaces**: Log distributions, weighted choices, precise control\n",
    "- **Conditional spaces**: Parameters that depend on other parameters\n",
    "- **Overrides**: After initial experiments to focus on promising regions\n",
    "\n",
    "### üéØ Key Takeaway\n",
    "SpaX gives you powerful search space exploration capabilities with almost no code changes. Start with automatic inference, add explicit spaces where needed, use conditional spaces for dependencies, and leverage overrides to iteratively refine your search.\n",
    "\n",
    "**Happy exploring! üöÄ**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
